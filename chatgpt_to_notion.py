import os
import re
import time
import json
import glob
import zipfile
import argparse
from datetime import datetime, timezone, timedelta
from collections import defaultdict

import requests
from dotenv import load_dotenv

# OpenAIï¼ˆè¦ç´„ã‚’ä½¿ã†å ´åˆã®ã¿ï¼‰
try:
    from openai import OpenAI
except Exception:
    OpenAI = None  # OPENAIã‚’ä½¿ã‚ãªã„é‹ç”¨ã§ã‚‚å‹•ã

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨é€±å ±æ©Ÿèƒ½ã‚’daily_chatgpt_summaryã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from daily_chatgpt_summary import (
    SYSTEM_PROMPT, USER_PROMPT_TEMPLATE,
    should_create_weekly_report, get_weekly_date_range,
    create_weekly_report, get_last_weekly_report_date,
    save_last_weekly_report_date, has_sufficient_weekly_data,
    jst_today
)

JST = timezone(timedelta(hours=9))
STATE_FILE = "state.json"


def jst_now_iso(): return datetime.now(JST).isoformat()
def jst_today(): return datetime.now(JST).date().isoformat()

# ---------- state.jsonï¼ˆå‰å›ä»¥é™ã®ã¿å‡¦ç†ã™ã‚‹"ã—ãŠã‚Š"ï¼‰ ----------


def load_state(path):
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    return {"version": 1, "conv_hwm": {}, "seen": {}}


def save_state(path, state):
    tmp = path + ".tmp"
    with open(tmp, "w", encoding="utf-8") as f:
        json.dump(state, f, ensure_ascii=False, indent=2)
    os.replace(tmp, path)

# ---------- ZIPãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ ----------


def get_downloads_dir():
    d = os.getenv("DOWNLOADS_DIR")
    if d:
        return d
    return os.path.join(os.path.expanduser("~"), "Downloads")

# ---------- ZIP â†’ conversations.json ----------


def unzip_to_tmp(zip_path, workdir):
    tmp = os.path.join(workdir, "_tmp_unzip")
    if os.path.exists(tmp):
        for f in glob.glob(os.path.join(tmp, "*")):
            try:
                if os.path.isdir(f):
                    import shutil
                    shutil.rmtree(f, ignore_errors=True)
                else:
                    os.remove(f)
            except Exception:
                pass
    os.makedirs(tmp, exist_ok=True)
    with zipfile.ZipFile(zip_path) as zf:
        zf.extractall(tmp)
    return os.path.join(tmp, "conversations.json")


def load_conversations(path):
    if not os.path.exists(path):
        raise FileNotFoundError("conversations.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå†…å®¹ã‚’ç¢ºèªï¼‰ã€‚")
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    if isinstance(data, dict) and "conversations" in data:
        return data["conversations"]
    return data

# ---------- ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ§‹é€ ã®èª­ã¿å‡ºã— ----------


def iter_messages(conv):
    """
    mapping æ§‹é€ ã‚’æƒ³å®šã€‚ç„¡ã‘ã‚Œã° messages é…åˆ—ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‚
    return: (conv_id, role, text, ts(UTCç§’/None), mid, title)
    """
    conv_id = conv.get("id") or conv.get("conversation_id") or "(unknown)"
    title = conv.get("title") or ""
    mapping = conv.get("mapping")
    if mapping:
        for node_id, node in mapping.items():
            msg = (node.get("message") or {})
            role = (msg.get("author") or {}).get("role")
            content = msg.get("content")
            if not role or not content:
                continue
            text = None
            if isinstance(content, dict) and "parts" in content:
                parts = content.get("parts") or []
                text = "\n".join(p for p in parts if isinstance(p, str))
            elif isinstance(content, str):
                text = content
            if text:
                ts = msg.get("create_time")
                mid = msg.get("id") or f"mapping-{node_id}"
                yield conv_id, role, text, ts, mid, title
    else:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: messagesé…åˆ—
        messages = conv.get("messages", [])
        for m in messages:
            role = m.get("author", {}).get("role")
            content = m.get("content")
            text = None
            if isinstance(content, dict) and "parts" in content:
                parts = content.get("parts") or []
                text = "\n".join(p for p in parts if isinstance(p, str))
            elif isinstance(content, str):
                text = content
            ts = m.get("create_time")
            if role and text:
                mid = m.get("id") or f"legacy-{hash((role,text,ts))}"
                yield conv_id, role, text, ts, mid, title


def ts_to_day(ts, from_date_str="2025-09-25"):
    if ts is None:
        return jst_today()
    try:
        date_obj = datetime.fromtimestamp(
            ts, tz=timezone.utc).astimezone(JST).date()
        from_date = datetime.strptime(from_date_str, "%Y-%m-%d").date()
        # æŒ‡å®šæ—¥ä»¥é™ã®æ—¥ä»˜ã®ã¿ã‚’å‡¦ç†
        if date_obj >= from_date:
            return date_obj.isoformat()
        else:
            return None  # æŒ‡å®šæ—¥ä»¥å‰ã¯é™¤å¤–
    except Exception:
        return jst_today()


def build_daily_raw(conversations, state, max_chars=16000, from_date_str="2025-09-25"):
    """
    stateï¼ˆconv_hwm/seenï¼‰ã‚’ä½¿ã£ã¦"å‰å›ä»¥é™ã®ã¿"ã‚’æ—¥ä»˜ã”ã¨ã«ã¾ã¨ã‚ã‚‹
    ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¯¾å¿œ: ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ™ãƒ¼ã‚¹ã®åˆ¤å®šã‚’å„ªå…ˆ
    return: (day -> raw_text, progress: conv_id -> (new_seen_ids, max_ts))
    """
    hwm = state["conv_hwm"]
    seen = state["seen"]

    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œã®stateã‹ã©ã†ã‹ã‚’åˆ¤å®š
    is_optimized_state = state.get("version", 1) >= 2
    if is_optimized_state:
        print(f"[INFO] æœ€é©åŒ–ã•ã‚ŒãŸstate.jsonã‚’ä½¿ç”¨ä¸­ï¼ˆãƒãƒ¼ã‚¸ãƒ§ãƒ³: {state.get('version', 1)}ï¼‰")
        print(f"[INFO] ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ™ãƒ¼ã‚¹ã®åˆ¤å®šã‚’å„ªå…ˆã—ã¾ã™")
    buckets = defaultdict(list)
    progress = {}

    for conv in conversations:
        last_ts_map = hwm
        conv_id = conv.get("id") or conv.get("conversation_id") or "(unknown)"
        last_ts = last_ts_map.get(conv_id, -1)
        seen_set = set(seen.get(conv_id, []))
        new_seen = set()
        max_ts = last_ts

        for cid, role, text, ts, mid, title in iter_messages(conv):
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ™ãƒ¼ã‚¹ã®åˆ¤å®šã‚’å„ªå…ˆï¼ˆã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¯¾å¿œï¼‰
            if ts is not None and ts <= last_ts:
                continue

            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸IDãƒ™ãƒ¼ã‚¹ã®åˆ¤å®šï¼ˆé‡è¤‡å›é¿ï¼‰
            if mid in seen_set:
                continue
            day = ts_to_day(ts, from_date_str)
            if day is None:
                # æŒ‡å®šæ—¥ä»¥å‰ã®æ—¥ä»˜ã¯ã‚¹ã‚­ãƒƒãƒ—
                continue
            prefix = "ãƒ¦ãƒ¼ã‚¶ãƒ¼" if role == "user" else (
                "ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ" if role == "assistant" else role)
            buckets[day].append(f"[{prefix}] {title}ï½œ{text.strip()}")
            new_seen.add(mid)
            if ts is not None and ts > max_ts:
                max_ts = ts

        if new_seen:
            progress[conv_id] = (new_seen, max_ts)

    daily = {}
    for day, lines in buckets.items():
        joined = "\n- " + "\n- ".join(lines)
        if len(joined) > max_chars:
            joined = joined[:max_chars] + "\nâ€¦ï¼ˆé•·æ–‡ã®ãŸã‚é€”ä¸­ã¾ã§ï¼‰"
        daily[day] = joined
    return daily, progress


def build_daily_raw_all_data(conversations, max_chars=16000, from_date_str="2025-09-25"):
    """
    ãƒ†ã‚¹ãƒˆç”¨: å…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ï¼ˆå·®åˆ†å‡¦ç†ãªã—ï¼‰
    return: (day -> raw_text, progress: conv_id -> (new_seen_ids, max_ts))
    """
    buckets = defaultdict(list)
    progress = {}

    for conv in conversations:
        conv_id = conv.get("id") or conv.get("conversation_id") or "(unknown)"
        seen_set = set()  # ç©ºã®ã‚»ãƒƒãƒˆï¼ˆå…¨ã¦å‡¦ç†ï¼‰
        new_seen = set()
        max_ts = 0

        for cid, role, text, ts, mid, title in iter_messages(conv):
            # å…¨ã¦ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡¦ç†ï¼ˆå·®åˆ†ãƒã‚§ãƒƒã‚¯ãªã—ï¼‰
            day = ts_to_day(ts, from_date_str)
            if day is None:
                # æŒ‡å®šæ—¥ä»¥å‰ã®æ—¥ä»˜ã¯ã‚¹ã‚­ãƒƒãƒ—
                continue
            prefix = "ãƒ¦ãƒ¼ã‚¶ãƒ¼" if role == "user" else (
                "ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ" if role == "assistant" else role)
            buckets[day].append(f"[{prefix}] {title}ï½œ{text.strip()}")
            new_seen.add(mid)
            if ts is not None and ts > max_ts:
                max_ts = ts

        if new_seen:
            progress[conv_id] = (new_seen, max_ts)

    daily = {}
    for day, lines in buckets.items():
        joined = "\n- " + "\n- ".join(lines)
        if len(joined) > max_chars:
            joined = joined[:max_chars] + "\nâ€¦ï¼ˆé•·æ–‡ã®ãŸã‚é€”ä¸­ã¾ã§ï¼‰"
        daily[day] = joined
    return daily, progress


# ---------- è¦ç´„ï¼ˆä»»æ„ï¼‰ ----------
PROMPT = """ä»¥ä¸‹ã®ChatGPTç›¸è«‡ãƒ­ã‚°ã‚’åˆ†æã—ã€æ—¥ä»˜ã”ã¨ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚

é‡è¦ãªæŒ‡ç¤ºï¼š
1. ç”Ÿã®ãƒ­ã‚°ã‚’ãã®ã¾ã¾å‡ºåŠ›ã—ã¦ã¯ã„ã‘ã¾ã›ã‚“
2. å¿…ãšä»¥ä¸‹ã®å½¢å¼ã§è¦ç´„ã—ã¦ãã ã•ã„
3. å„æ—¥ã®ç›¸è«‡å†…å®¹ã‚’3-5å€‹ã®ãƒˆãƒ”ãƒƒã‚¯ã«æ•´ç†ã—ã¦ãã ã•ã„

å‡ºåŠ›å½¢å¼ï¼ˆå³å®ˆï¼‰ï¼š
## æ—¥ä»˜ï¼ˆYYYY-MM-DDï¼‰

### ã€”ç›¸è«‡ã—ãŸãƒˆãƒ”ãƒƒã‚¯åã€•
**è¦ç‚¹:** ç›¸è«‡ã®è¦ç‚¹ã‚’ç°¡æ½”ã«ã¾ã¨ã‚
**æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:** ä»Šå¾Œã®èª¿æŸ»ãƒ»å­¦ç¿’ã™ã¹ãå†…å®¹

ï¼ˆä»¥ä¸‹ã€ä»–ã®æ—¥ä»˜ã«ã¤ã„ã¦ã‚‚åŒæ§˜ã®å½¢å¼ã§ç¶šã‘ã‚‹ï¼‰

ç›¸è«‡ãƒ­ã‚°ï¼š
{notes}
"""


def summarize(raw_text, api_key, model):
    print(f"[DEBUG] ===== summarizeé–¢æ•°å‘¼ã³å‡ºã— =====")
    print(f"[DEBUG] OpenAI: {'åˆ©ç”¨å¯èƒ½' if OpenAI else 'åˆ©ç”¨ä¸å¯'}")
    print(f"[DEBUG] ãƒ¢ãƒ‡ãƒ«: {model}")
    print(f"[DEBUG] å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(raw_text)}æ–‡å­—")

    if not api_key or not OpenAI:
        # è¦ç´„ã—ãªã„ï¼ˆç”Ÿãƒ­ã‚°ã‚’ä¿å­˜ï¼‰
        print(f"[DEBUG] OpenAIãŒæœªè¨­å®šã®ãŸã‚ã€ç”Ÿãƒ­ã‚°ã‚’è¿”ã—ã¾ã™")
        day = jst_today()
        return f"## ğŸ“… {day} ChatGPTæŒ¯ã‚Šè¿”ã‚Šï¼ˆç”Ÿãƒ­ã‚°ï¼‰\n\n```\n{raw_text}\n```"

    # ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨
    user_prompt = USER_PROMPT_TEMPLATE.format(raw_text=raw_text)

    print(f"ğŸ“¤ ChatGPT APIæŠ•ã’ã¾ã—ãŸ:")
    print(f"   ãƒ¢ãƒ‡ãƒ«: {model}")
    print(f"   å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(raw_text)}æ–‡å­—")
    print(f"   å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®å…ˆé ­: {raw_text[:300]}...")

    client = OpenAI(api_key=api_key)
    resp = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt}
        ],
        temperature=0.3  # ã‚ˆã‚Šä¸€è²«ã—ãŸå‡ºåŠ›ã®ãŸã‚
    )

    result = resp.choices[0].message.content.strip()

    print(f"ğŸ“¥ ChatGPTã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹:")
    print(f"   å‡ºåŠ›é•·: {len(result)}æ–‡å­—")
    print(
        f"   ä½¿ç”¨ãƒˆãƒ¼ã‚¯ãƒ³: {resp.usage.total_tokens if hasattr(resp, 'usage') else 'ä¸æ˜'}")
    print(f"   '##'ã®æ•°: {result.count('##')}")
    print(f"   '###'ã®æ•°: {result.count('###')}")
    print(f"   å†…å®¹: {result[:300]}..." if len(
        result) > 300 else f"   å†…å®¹: {result}")

    # ãƒˆãƒ”ãƒƒã‚¯é–“ã®æ”¹è¡Œã‚’æ•´å½¢
    # ### ã®å‰ã«æ”¹è¡ŒãŒãªã„å ´åˆã¯è¿½åŠ 
    result = re.sub(r'([^\n])\n(###\s)', r'\1\n\n\2', result)
    # ### ã®ç›´å¾Œã«æ”¹è¡ŒãŒ1ã¤ã—ã‹ãªã„å ´åˆã‚‚èª¿æ•´
    result = re.sub(r'(###[^\n]+)\n([^\n])', r'\1\n\n\2', result)

    print(f"ğŸ“ æ•´å½¢å¾Œ:")
    print(f"   å‡ºåŠ›é•·: {len(result)}æ–‡å­—")

    return result

# ---------- Notion ----------


def markdown_to_notion_blocks(markdown_text, max_chars_per_block=1900):
    """
    Markdownãƒ†ã‚­ã‚¹ãƒˆã‚’Notionãƒ–ãƒ­ãƒƒã‚¯å½¢å¼ã«å¤‰æ›ï¼ˆå¤ªå­—è¨˜æ³•å¯¾å¿œï¼‰
    """
    import re

    def parse_rich_text(text):
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’è§£æã—ã¦Notionã®rich_textå½¢å¼ã«å¤‰æ›"""
        rich_text = []
        current_pos = 0

        # **å¤ªå­—** ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
        bold_pattern = r'\*\*(.*?)\*\*'
        for match in re.finditer(bold_pattern, text):
            # å¤ªå­—ã®å‰ã®ãƒ†ã‚­ã‚¹ãƒˆ
            if match.start() > current_pos:
                normal_text = text[current_pos:match.start()]
                if normal_text:
                    rich_text.append({
                        "type": "text",
                        "text": {"content": normal_text}
                    })

            # å¤ªå­—éƒ¨åˆ†
            bold_text = match.group(1)
            rich_text.append({
                "type": "text",
                "text": {"content": bold_text},
                "annotations": {"bold": True}
            })

            current_pos = match.end()

        # æ®‹ã‚Šã®ãƒ†ã‚­ã‚¹ãƒˆ
        if current_pos < len(text):
            remaining_text = text[current_pos:]
            if remaining_text:
                rich_text.append({
                    "type": "text",
                    "text": {"content": remaining_text}
                })

        return rich_text if rich_text else [{"type": "text", "text": {"content": text}}]

    blocks = []
    lines = markdown_text.split('\n')
    current_content = ""

    for line_num, line in enumerate(lines, 1):

        # è¦‹å‡ºã—ã®å‡¦ç†ï¼ˆå³å¯†ãªæ¡ä»¶ã§åˆ¤å®šï¼‰
        is_heading3 = line.startswith('###') and not line.startswith('####')
        is_heading2 = line.startswith('##') and not line.startswith('###')
        is_heading1 = line.startswith('#') and not line.startswith(
            '##') and not line.startswith('###')

        if is_heading3:
            # ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒã‚ã‚‹å ´åˆã¯ä¿å­˜
            if current_content.strip():
                blocks.append({
                    "object": "block",
                    "type": "paragraph",
                    "paragraph": {
                        "rich_text": parse_rich_text(current_content.strip())
                    }
                })
                current_content = ""

            # å°è¦‹å‡ºã—ãƒ–ãƒ­ãƒƒã‚¯ã‚’è¿½åŠ ï¼ˆ###ã®å¾Œã®ç©ºç™½ã‚’é©åˆ‡ã«å‡¦ç†ã€ç©ºã®è¦‹å‡ºã—ã¯ç„¡è¦–ï¼‰
            heading_text = line[3:].strip()  # "###" ã‚’é™¤å»ã—ã¦ã‹ã‚‰strip
            if heading_text.startswith(' '):
                heading_text = heading_text[1:]  # å…ˆé ­ã®ç©ºç™½ã‚’é™¤å»

            # ç©ºã®è¦‹å‡ºã—ã¯ä½œæˆã—ãªã„
            if heading_text:
                blocks.append({
                    "object": "block",
                    "type": "heading_3",
                    "heading_3": {
                        "rich_text": parse_rich_text(heading_text)
                    }
                })
                # ãƒˆãƒ”ãƒƒã‚¯é–“ã®è¡Œé–“ã‚’ç©ºã‘ã‚‹ãŸã‚ã«ç©ºã®æ®µè½ã‚’2ã¤è¿½åŠ 
                blocks.append({
                    "object": "block",
                    "type": "paragraph",
                    "paragraph": {
                        "rich_text": [{"type": "text", "text": {"content": ""}}]
                    }
                })
                blocks.append({
                    "object": "block",
                    "type": "paragraph",
                    "paragraph": {
                        "rich_text": [{"type": "text", "text": {"content": ""}}]
                    }
                })
        elif is_heading2:
            # ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒã‚ã‚‹å ´åˆã¯ä¿å­˜
            if current_content.strip():
                blocks.append({
                    "object": "block",
                    "type": "paragraph",
                    "paragraph": {
                        "rich_text": parse_rich_text(current_content.strip())
                    }
                })
                current_content = ""

            # è¦‹å‡ºã—ãƒ–ãƒ­ãƒƒã‚¯ã‚’è¿½åŠ ï¼ˆç©ºã®è¦‹å‡ºã—ã¯ç„¡è¦–ï¼‰
            heading_text = line[2:].strip()  # "##" ã‚’é™¤å»ã—ã¦ã‹ã‚‰strip
            if heading_text.startswith(' '):
                heading_text = heading_text[1:]  # å…ˆé ­ã®ç©ºç™½ã‚’é™¤å»

            # ç©ºã®è¦‹å‡ºã—ã¯ä½œæˆã—ãªã„
            if heading_text:
                blocks.append({
                    "object": "block",
                    "type": "heading_2",
                    "heading_2": {
                        "rich_text": parse_rich_text(heading_text)
                    }
                })
                # ãƒˆãƒ”ãƒƒã‚¯é–“ã®è¡Œé–“ã‚’ç©ºã‘ã‚‹ãŸã‚ã«ç©ºã®æ®µè½ã‚’2ã¤è¿½åŠ 
                blocks.append({
                    "object": "block",
                    "type": "paragraph",
                    "paragraph": {
                        "rich_text": [{"type": "text", "text": {"content": ""}}]
                    }
                })
                blocks.append({
                    "object": "block",
                    "type": "paragraph",
                    "paragraph": {
                        "rich_text": [{"type": "text", "text": {"content": ""}}]
                    }
                })
        elif is_heading1:
            # ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒã‚ã‚‹å ´åˆã¯ä¿å­˜
            if current_content.strip():
                blocks.append({
                    "object": "block",
                    "type": "paragraph",
                    "paragraph": {
                        "rich_text": parse_rich_text(current_content.strip())
                    }
                })
                current_content = ""

            # å¤§è¦‹å‡ºã—ãƒ–ãƒ­ãƒƒã‚¯ã‚’è¿½åŠ ï¼ˆç©ºã®è¦‹å‡ºã—ã¯ç„¡è¦–ï¼‰
            heading_text = line[1:].strip()  # "#" ã‚’é™¤å»ã—ã¦ã‹ã‚‰strip
            if heading_text.startswith(' '):
                heading_text = heading_text[1:]  # å…ˆé ­ã®ç©ºç™½ã‚’é™¤å»

            # ç©ºã®è¦‹å‡ºã—ã¯ä½œæˆã—ãªã„
            if heading_text:
                blocks.append({
                    "object": "block",
                    "type": "heading_1",
                    "heading_1": {
                        "rich_text": parse_rich_text(heading_text)
                    }
                })
                # ãƒˆãƒ”ãƒƒã‚¯é–“ã®è¡Œé–“ã‚’ç©ºã‘ã‚‹ãŸã‚ã«ç©ºã®æ®µè½ã‚’2ã¤è¿½åŠ 
                blocks.append({
                    "object": "block",
                    "type": "paragraph",
                    "paragraph": {
                        "rich_text": [{"type": "text", "text": {"content": ""}}]
                    }
                })
                blocks.append({
                    "object": "block",
                    "type": "paragraph",
                    "paragraph": {
                        "rich_text": [{"type": "text", "text": {"content": ""}}]
                    }
                })
        else:
            # é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆç©ºè¡Œã¯ç„¡è¦–ï¼‰
            if line.strip():  # ç©ºè¡Œã§ãªã„å ´åˆã®ã¿è¿½åŠ 
                # ã€Œæ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã€ã®è¡Œã®å ´åˆã¯è¿½åŠ ã®æ”¹è¡Œã‚’å…¥ã‚Œã‚‹
                if line.strip().startswith('**æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:**'):
                    current_content += line + '\n\n'  # è¿½åŠ ã®æ”¹è¡Œ
                else:
                    current_content += line + '\n'

            # æ–‡å­—æ•°åˆ¶é™ãƒã‚§ãƒƒã‚¯
            if len(current_content) > max_chars_per_block:
                # ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã¨ã—ã¦ä¿å­˜
                blocks.append({
                    "object": "block",
                    "type": "paragraph",
                    "paragraph": {
                        "rich_text": parse_rich_text(current_content.strip())
                    }
                })
                current_content = ""

    # æ®‹ã‚Šã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒã‚ã‚‹å ´åˆã¯ä¿å­˜
    if current_content.strip():
        blocks.append({
            "object": "block",
            "type": "paragraph",
            "paragraph": {
                "rich_text": parse_rich_text(current_content.strip())
            }
        })

    return blocks


def notion_get_title_date_props(token, dbid):
    h = {"Authorization": f"Bearer {token}", "Notion-Version": "2022-06-28"}
    r = requests.get(f"https://api.notion.com/v1/databases/{dbid}", headers=h)
    r.raise_for_status()
    data = r.json()
    title_prop = next(
        k for k, v in data["properties"].items() if v["type"] == "title")
    date_prop = next(
        (k for k, v in data["properties"].items() if v["type"] == "date"), None)
    return title_prop, date_prop


def notion_create_page(token, dbid, title_prop, date_prop, day, markdown):
    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/json",
        "Notion-Version": "2022-06-28",
    }
    props = {title_prop: {
        "title": [{"text": {"content": f"{day} ChatGPTæŒ¯ã‚Šè¿”ã‚Š"}}]}}
    if date_prop:
        # 3æ—¥å¾Œã®12:00ã€œ13:00ï¼ˆJSTï¼‰ã‚’è¨­å®š
        date_obj = datetime.strptime(day, "%Y-%m-%d").date()
        next_day = date_obj + timedelta(days=3)

        # UTCã§æ—¥æœ¬æ™‚é–“12-13æ™‚ã‚’æŒ‡å®šï¼ˆUTC = JST - 9æ™‚é–“ï¼‰
        start_time_utc = datetime.combine(next_day, datetime.min.time().replace(
            hour=12, minute=0)).replace(tzinfo=timezone.utc)  # JST 12:00 = UTC 03:00
        end_time_utc = datetime.combine(next_day, datetime.min.time().replace(
            hour=13, minute=0)).replace(tzinfo=timezone.utc)  # JST 13:00 = UTC 04:00

        props[date_prop] = {
            "date": {
                "start": start_time_utc.isoformat(),
                "end": end_time_utc.isoformat(),
                "time_zone": "Asia/Tokyo"
            }
        }
    # Markdownã‚’Notionãƒ–ãƒ­ãƒƒã‚¯å½¢å¼ã«å¤‰æ›
    children = markdown_to_notion_blocks(markdown)

    # ãƒ‡ãƒãƒƒã‚°ç”¨: å¤‰æ›çµæœã‚’è¡¨ç¤º
    print(f"[DEBUG] å¤‰æ›ã•ã‚ŒãŸãƒ–ãƒ­ãƒƒã‚¯æ•°: {len(children)}")
    print(f"[DEBUG] å…ƒã®Markdownãƒ†ã‚­ã‚¹ãƒˆ: {repr(markdown[:200])}")
    for i, block in enumerate(children[:5]):  # æœ€åˆã®5ãƒ–ãƒ­ãƒƒã‚¯ã‚’è¡¨ç¤º
        block_type = block.get('type', 'unknown')
        if block_type == 'heading_3':
            content = block.get('heading_3', {}).get('rich_text', [{}])[
                0].get('text', {}).get('content', '')
            print(f"[DEBUG] ãƒ–ãƒ­ãƒƒã‚¯{i+1}: {block_type} - å†…å®¹: {repr(content)}")
        elif block_type == 'paragraph':
            content = block.get('paragraph', {}).get('rich_text', [{}])[
                0].get('text', {}).get('content', '')
            print(
                f"[DEBUG] ãƒ–ãƒ­ãƒƒã‚¯{i+1}: {block_type} - å†…å®¹: {repr(content[:50])}")
        else:
            print(f"[DEBUG] ãƒ–ãƒ­ãƒƒã‚¯{i+1}: {block_type} - {str(block)[:100]}...")

    payload = {"parent": {"database_id": dbid},
               "properties": props, "children": children}

    # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
    print(f"[DEBUG] ===== ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ =====")
    print(f"[DEBUG] ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£: {props}")
    print(f"[DEBUG] å­ãƒ–ãƒ­ãƒƒã‚¯æ•°: {len(children)}")

    # ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã®æ¤œè¨¼
    if not props.get(title_prop):
        print(f"[ERROR] ã‚¿ã‚¤ãƒˆãƒ«ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ '{title_prop}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return None

    # å­ãƒ–ãƒ­ãƒƒã‚¯ã®æ¤œè¨¼
    if not children:
        print(f"[WARNING] å­ãƒ–ãƒ­ãƒƒã‚¯ãŒç©ºã§ã™")
    else:
        print(f"[DEBUG] å­ãƒ–ãƒ­ãƒƒã‚¯è©³ç´°:")
        for i, child in enumerate(children[:5]):  # æœ€åˆã®5ã¤ã ã‘è¡¨ç¤º
            block_type = child.get('type', 'unknown')
            if block_type == 'heading_1':
                content = child.get('heading_1', {}).get('rich_text', [{}])[
                    0].get('text', {}).get('content', '')
                print(f"[DEBUG]   ãƒ–ãƒ­ãƒƒã‚¯{i+1}: heading_1 - '{content}'")
            elif block_type == 'heading_2':
                content = child.get('heading_2', {}).get('rich_text', [{}])[
                    0].get('text', {}).get('content', '')
                print(f"[DEBUG]   ãƒ–ãƒ­ãƒƒã‚¯{i+1}: heading_2 - '{content}'")
            elif block_type == 'heading_3':
                content = child.get('heading_3', {}).get('rich_text', [{}])[
                    0].get('text', {}).get('content', '')
                print(f"[DEBUG]   ãƒ–ãƒ­ãƒƒã‚¯{i+1}: heading_3 - '{content}'")
            elif block_type == 'paragraph':
                content = child.get('paragraph', {}).get('rich_text', [{}])[
                    0].get('text', {}).get('content', '')
                print(
                    f"[DEBUG]   ãƒ–ãƒ­ãƒƒã‚¯{i+1}: paragraph - '{content[:100]}{'...' if len(content) > 100 else ''}'")
            else:
                print(
                    f"[DEBUG]   ãƒ–ãƒ­ãƒƒã‚¯{i+1}: {block_type} - {str(child)[:100]}...")

    # ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã‚µã‚¤ã‚ºã®æ¤œè¨¼
    payload_size = len(str(payload))
    print(f"[DEBUG] ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã‚µã‚¤ã‚º: {payload_size}æ–‡å­—")
    if payload_size > 100000:  # 100KBåˆ¶é™
        print(f"[WARNING] ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã‚µã‚¤ã‚ºãŒå¤§ãã™ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")

    print(f"[DEBUG] ========================")

    r = requests.post("https://api.notion.com/v1/pages",
                      headers=headers, json=payload)

    # ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®è©³ç´°ã‚’è¡¨ç¤º
    if r.status_code != 200:
        print(f"[ERROR] Notion API ã‚¨ãƒ©ãƒ¼: {r.status_code}")
        print(f"[ERROR] ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {r.text}")
        try:
            error_detail = r.json()
            print(f"[ERROR] è©³ç´°: {error_detail}")
        except:
            pass

        # ã‚¨ãƒ©ãƒ¼è©³ç´°ã‚’ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚‚è¨˜éŒ²
        error_log = f"Notion API Error {r.status_code}: {r.text}"
        print(f"[ERROR] ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°: {error_log}")

        # 400ã‚¨ãƒ©ãƒ¼ã®å ´åˆã€ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã®è©³ç´°ã‚‚è¡¨ç¤º
        if r.status_code == 400:
            print(f"[ERROR] é€ä¿¡ã•ã‚ŒãŸãƒšã‚¤ãƒ­ãƒ¼ãƒ‰:")
            print(f"[ERROR] ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£: {props}")
            print(f"[ERROR] å­ãƒ–ãƒ­ãƒƒã‚¯æ•°: {len(children)}")
            print(f"[ERROR] æœ€åˆã®3ã¤ã®å­ãƒ–ãƒ­ãƒƒã‚¯:")
            for i, child in enumerate(children[:3]):
                print(f"[ERROR]   ãƒ–ãƒ­ãƒƒã‚¯{i+1}: {child}")

        # ã‚¨ãƒ©ãƒ¼ã‚’å†ç™ºç”Ÿã•ã›ãšã«Noneã‚’è¿”ã™
        return None

    r.raise_for_status()
    return r.json().get("url")


def notion_create_weekly_page(token, dbid, title_prop, date_prop, title, date_with_time, markdown):
    """é€±å ±ç”¨ã®Notionãƒšãƒ¼ã‚¸ä½œæˆ"""
    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/json",
        "Notion-Version": "2022-06-28",
    }
    props = {title_prop: {
        "title": [{"text": {"content": title}}]}}
    if date_prop:
        props[date_prop] = {"date": date_with_time}

    # Markdownã‚’Notionãƒ–ãƒ­ãƒƒã‚¯å½¢å¼ã«å¤‰æ›
    children = markdown_to_notion_blocks(markdown)

    payload = {"parent": {"database_id": dbid},
               "properties": props, "children": children}

    r = requests.post("https://api.notion.com/v1/pages", headers=headers,
                      data=json.dumps(payload, ensure_ascii=False).encode("utf-8"))
    if r.status_code not in (200, 201):
        raise RuntimeError(f"é€±å ±ãƒšãƒ¼ã‚¸ä½œæˆã«å¤±æ•—: {r.status_code} {r.text}")
    return r.json()

# ---------- é€±é–“çµ±è¨ˆå–å¾— ----------


def get_weekly_conversations_with_stats(conversations, monday, friday):
    """æŒ‡å®šæœŸé–“ã®ä¼šè©±ãƒ­ã‚°ã¨çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
    from datetime import datetime, timezone

    # æ—¥ä»˜ç¯„å›²ã‚’datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
    monday_dt = datetime.strptime(
        monday, "%Y-%m-%d").replace(tzinfo=timezone.utc)
    friday_dt = datetime.strptime(
        friday, "%Y-%m-%d").replace(tzinfo=timezone.utc)

    # çµ±è¨ˆæƒ…å ±ã®åˆæœŸåŒ–
    stats = {
        'conversation_count': 0,
        'user_message_count': 0,
        'assistant_message_count': 0,
        'total_duration_minutes': 0.0
    }

    weekly_conversations = []

    for conv in conversations:
        # ä¼šè©±IDã‚’å–å¾—
        conv_id = conv.get("id") or conv.get("conversation_id") or "(unknown)"

        # ä¼šè©±ã®ä½œæˆæ—¥æ™‚ã‚’ç¢ºèª
        create_time = conv['create_time']
        if isinstance(create_time, float):
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å½¢å¼ã®å ´åˆ
            created_time = datetime.fromtimestamp(create_time, tz=timezone.utc)
        elif isinstance(create_time, str):
            # ISOæ–‡å­—åˆ—å½¢å¼ã®å ´åˆ
            created_time = datetime.fromisoformat(
                create_time.replace('Z', '+00:00'))
        else:
            continue  # ä¸æ˜ãªå½¢å¼ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—

        # æŒ‡å®šæœŸé–“å†…ã®ä¼šè©±ã®ã¿å‡¦ç†
        if monday_dt <= created_time <= friday_dt:
            stats['conversation_count'] += 1

            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            for message in conv.get('mapping', {}).values():
                if message.get('message'):
                    author = message['message'].get(
                        'author', {}).get('role', '')
                    if author == 'user':
                        stats['user_message_count'] += 1
                    elif author == 'assistant':
                        stats['assistant_message_count'] += 1

            # ä¼šè©±æ™‚é–“ã‚’è¨ˆç®—ï¼ˆæœ€åˆã¨æœ€å¾Œã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æ™‚é–“å·®ï¼‰
            message_times = []
            for message in conv.get('mapping', {}).values():
                if message.get('message') and message['message'].get('create_time'):
                    msg_create_time = message['message']['create_time']
                    if isinstance(msg_create_time, float):
                        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å½¢å¼ã®å ´åˆ
                        msg_time = datetime.fromtimestamp(
                            msg_create_time, tz=timezone.utc)
                    elif isinstance(msg_create_time, str):
                        # ISOæ–‡å­—åˆ—å½¢å¼ã®å ´åˆ
                        msg_time = datetime.fromisoformat(
                            msg_create_time.replace('Z', '+00:00'))
                    else:
                        continue  # ä¸æ˜ãªå½¢å¼ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                    message_times.append(msg_time)

            if len(message_times) >= 2:
                duration = (max(message_times) - min(message_times)
                            ).total_seconds() / 60
                stats['total_duration_minutes'] += duration

            # ä¼šè©±å†…å®¹ã‚’ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§ä¿å­˜
            conv_text = f"=== ä¼šè©± {conv_id} ({created_time.strftime('%Y-%m-%d %H:%M')}) ===\n"
            for message in conv.get('mapping', {}).values():
                if message.get('message'):
                    author = message['message'].get(
                        'author', {}).get('role', '')
                    content = message['message'].get(
                        'content', {}).get('parts', [''])[0]
                    if author == 'user':
                        conv_text += f"[ãƒ¦ãƒ¼ã‚¶ãƒ¼] {content}\n"
                    elif author == 'assistant':
                        conv_text += f"[ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ] {content}\n"
            conv_text += "\n"

            weekly_conversations.append(conv_text)

    # é€±é–“ãƒ­ã‚°ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
    weekly_raw_text = f"=== é€±é–“ç›¸è«‡ãƒ­ã‚° ({monday} ã€œ {friday}) ===\n\n"
    weekly_raw_text += f"ğŸ“Š é€±é–“çµ±è¨ˆ:\n"
    weekly_raw_text += f"- ãƒãƒ£ãƒƒãƒˆä¼šè©±æ•°: {stats['conversation_count']}å›\n"
    weekly_raw_text += f"- ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°: {stats['user_message_count']}å›\n"
    weekly_raw_text += f"- ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°: {stats['assistant_message_count']}å›\n"
    weekly_raw_text += f"- ç·ä¼šè©±æ™‚é–“: {stats['total_duration_minutes']:.1f}åˆ†\n\n"
    weekly_raw_text += "".join(weekly_conversations)

    return weekly_raw_text, stats

# ---------- ãƒ¡ã‚¤ãƒ³ ----------


def main():
    load_dotenv()
    parser = argparse.ArgumentParser(
        description="ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å·®åˆ†é›†ç´„â†’Notionç™»éŒ²")
    parser.add_argument("zip_file", nargs="?", default=None,
                        help="ChatGPTã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆZIPãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆæœªæŒ‡å®šæ™‚ã¯è‡ªå‹•æ¤œç´¢ï¼‰")
    parser.add_argument("--workdir", default=os.getenv("WORK_DIR", "./ChatGPT_Notion"),
                        help="ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆstate.jsonãªã©ï¼‰")
    parser.add_argument("--from-date", default="2025-09-18",
                        help="å‡¦ç†ã™ã‚‹æ—¥ä»˜ã®é–‹å§‹æ—¥ï¼ˆYYYY-MM-DDã€æ—¢å®š: 2025-09-18ï¼‰")
    args = parser.parse_args()

    notion_token = os.getenv("NOTION_TOKEN")
    notion_dbid = os.getenv("DATABASE_ID")
    if not notion_token or not notion_dbid:
        raise SystemExit("NOTION_TOKEN / DATABASE_ID ãŒæœªè¨­å®šã§ã™ï¼ˆ.envï¼‰ã€‚")

    openai_key = os.getenv("OPENAI_API_KEY")
    openai_model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")

    # ãƒ‡ãƒãƒƒã‚°ç”¨: ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿çŠ¶æ³ã‚’è¡¨ç¤º
    print(f"[DEBUG] ===== ç’°å¢ƒå¤‰æ•°ç¢ºèª =====")
    print(f"[DEBUG] ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿å®Œäº†")
    print(f"[DEBUG] ========================")

    os.makedirs(args.workdir, exist_ok=True)
    state_path = os.path.join(args.workdir, STATE_FILE)
    state = load_state(state_path)

    # è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—æ©Ÿèƒ½ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    try:
        from state_cleanup import auto_cleanup_state
        state = auto_cleanup_state(state_path, state)
    except ImportError:
        print("[INFO] state_cleanupãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
    except Exception as e:
        print(f"[WARNING] è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

    # --- ZIPãƒ•ã‚¡ã‚¤ãƒ«ã®å–å¾— ---
    if args.zip_file:
        zip_path = args.zip_file
        if not os.path.exists(zip_path):
            raise SystemExit(f"æŒ‡å®šã•ã‚ŒãŸZIPãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {zip_path}")
        print(f"[OK] æŒ‡å®šZIP: {zip_path}")
    else:
        # è‡ªå‹•æ¤œç´¢: Downloadsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰æœ€æ–°ã®ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
        downloads = get_downloads_dir()
        print(f"[INFO] Downloadsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¤œç´¢ä¸­: {downloads}")

        # å…¨ã¦ã®ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã®è¦å‰‡æ€§ã«é–¢ä¿‚ãªãï¼‰
        zip_files = glob.glob(os.path.join(downloads, "*.zip"))

        if not zip_files:
            raise SystemExit(
                "ZIPãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚Downloadsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç¢ºèªã™ã‚‹ã‹ã€--zip-file ã§ç›´æ¥æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")

        # æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠï¼ˆæ›´æ–°æ—¥æ™‚ã§ã‚½ãƒ¼ãƒˆï¼‰
        zip_path = max(zip_files, key=os.path.getmtime)
        print(f"[OK] æœ€æ–°ZIP: {zip_path}")

    # --- ZIP â†’ conversations.json èª­ã¿è¾¼ã¿ ---
    conv_json = unzip_to_tmp(zip_path, args.workdir)
    conversations = load_conversations(conv_json)

    # å·®åˆ†å‡¦ç†ï¼ˆstateã¨æ¯”è¼ƒã—ã¦æ–°è¦ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿å‡¦ç†ï¼‰
    daily_raw, progress = build_daily_raw(
        conversations, state, from_date_str=args.from_date)

    if not daily_raw:
        print("[INFO] å‰å›ä»¥é™ã®æ–°è¦ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãªã—ã€‚")
        print("é€±å ±ä½œæˆåˆ¤æ–­ã‚’å®Ÿè¡Œã—ã¾ã™...")
        # æ—¥å ±å‡¦ç†ã¯ã‚¹ã‚­ãƒƒãƒ—ã€é€±å ±ä½œæˆåˆ¤æ–­ã®ã¿å®Ÿè¡Œ
    else:
        print(f"[INFO] {len(daily_raw)}æ—¥åˆ†ã®æ–°è¦ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡¦ç†ã—ã¾ã™ã€‚")

    # --- Notionãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ ---
    title_prop, date_prop = notion_get_title_date_props(
        notion_token, notion_dbid)

    # --- æ—¥å ±å‡¦ç†ï¼ˆæ–°è¦ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒã‚ã‚‹å ´åˆã®ã¿ï¼‰ ---
    if daily_raw:
        print("\n=== æ—¥å ±å‡¦ç† ===")
    # --- è¤‡æ•°æ—¥ã‚’ã¾ã¨ã‚ã¦è¦ç´„â†’Notionç™»éŒ²ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦åˆ¶é™å¯¾å¿œï¼‰ ---
    MAX_CHARS_PER_REQUEST = 120000  # å®‰å…¨ãƒãƒ¼ã‚¸ãƒ³ã‚’æŒãŸã›ãŸåˆ¶é™

    # å…¨æ—¥ã®å†…å®¹ã‚’çµåˆ
    print(f"[DEBUG] daily_rawã®å†…å®¹: {daily_raw}")
    print(f"[DEBUG] daily_rawã®ã‚­ãƒ¼æ•°: {len(daily_raw)}")

    combined_text = ""
    for day in sorted(daily_raw.keys()):
        combined_text += f"\n## {day}\n{daily_raw[day]}\n"

    print(f"[DEBUG] combined_textã®é•·ã•: {len(combined_text)}")
    print(f"[DEBUG] combined_textã®å…ˆé ­: {combined_text[:200]}...")

    # combined_textãŒç©ºã®å ´åˆã¯APIå‘¼ã³å‡ºã—ã‚’ã‚¹ã‚­ãƒƒãƒ—
    if len(combined_text.strip()) == 0:
        print("[INFO] å‡¦ç†å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã®ãŸã‚ã€APIå‘¼ã³å‡ºã—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        daily_summaries = {}
    elif len(combined_text) > MAX_CHARS_PER_REQUEST:
        # ãƒ†ã‚­ã‚¹ãƒˆãŒé•·ã™ãã‚‹å ´åˆã¯åˆ†å‰²
        print(f"[WARN] å…¨æ—¥ã®å†…å®¹ãŒé•·ã™ãã‚‹ãŸã‚åˆ†å‰²ã—ã¦å‡¦ç†ã—ã¾ã™...")
        # é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²
        chunks = []
        current_chunk = ""
        lines = combined_text.split('\n')

        for line in lines:
            if len(current_chunk + line) > MAX_CHARS_PER_REQUEST and current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = line + '\n'
            else:
                current_chunk += line + '\n'

        if current_chunk.strip():
            chunks.append(current_chunk.strip())

        # å„ãƒãƒ£ãƒ³ã‚¯ã‚’è¦ç´„ã—ã¦çµåˆ
        chunk_summaries = []
        for i, chunk in enumerate(chunks):
            print(f"ãƒãƒ£ãƒ³ã‚¯ {i+1}/{len(chunks)} ã‚’è¦ç´„ä¸­...")
            chunk_md = summarize(chunk, openai_key, openai_model)
            chunk_summaries.append(f"## éƒ¨åˆ† {i+1}\n{chunk_md}")

        # ãƒãƒ£ãƒ³ã‚¯ã®è¦ç´„ã‚’çµåˆ
        combined_md = f"## è¤‡æ•°æ—¥ ChatGPTæŒ¯ã‚Šè¿”ã‚Šï¼ˆåˆ†å‰²å‡¦ç†ï¼‰\n\n" + \
            "\n\n".join(chunk_summaries)
    else:
        # 1å›ã®APIã§è¦ç´„
        print(f"[INFO] {len(daily_raw)}æ—¥åˆ†ã‚’ã¾ã¨ã‚ã¦è¦ç´„ä¸­ï¼ˆOpenAI: {openai_model}ï¼‰...")
        combined_md = summarize(combined_text, openai_key, openai_model)

    # APIå‘¼ã³å‡ºã—å¾Œã®å‡¦ç†
    if 'combined_md' in locals():
        # è¦ç´„çµæœã‚’æ—¥ä»˜ã”ã¨ã«åˆ†å‰²
        print(f"ğŸ“‹ è¦ç´„çµæœã®åˆ†å‰²å‡¦ç†:")
        print(f"   è¦ç´„çµæœå…¨ä½“ã®é•·ã•: {len(combined_md)}æ–‡å­—")
        print(f"   è¦ç´„çµæœã®å…ˆé ­: {combined_md[:500]}...")

        # æ—¥ä»˜ã®è¦‹å‡ºã—ï¼ˆ## YYYY-MM-DDï¼‰ã§åˆ†å‰²ï¼ˆ### ã¯é™¤å¤–ï¼‰
        import re
        sections = re.split(r'\n(?=## \d{4}-\d{2}-\d{2})', combined_md)
        daily_summaries = {}

        print(f"   åˆ†å‰²ã•ã‚ŒãŸã‚»ã‚¯ã‚·ãƒ§ãƒ³æ•°: {len(sections)}")

        for i, section in enumerate(sections):
            print(f"   ã‚»ã‚¯ã‚·ãƒ§ãƒ³{i}: é•·ã•={len(section)}, å†…å®¹='{section[:100]}...'")

        print(f"ğŸ“‹ æ—¥ä»˜ã”ã¨ã«è¦ç´„çµæœã¾ã¨ã‚ã¾ã—ãŸ:")

        for i, section in enumerate(sections):
            section = section.strip()
            if not section:
                continue

            lines = section.split('\n')

            # æœ€åˆã®è¡Œã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡ºï¼ˆ## ã‚’é™¤å»ï¼‰
            first_line = lines[0].strip()
            if first_line.startswith('## '):
                day = first_line[3:].strip()  # "## " ã‚’é™¤å»
            elif re.match(r'^\d{4}-\d{2}-\d{2}', first_line):
                # "## " ãŒãªã„å ´åˆã§ã‚‚æ—¥ä»˜å½¢å¼ãªã‚‰å—ã‘å…¥ã‚Œã‚‹
                day = first_line
            else:
                print(f"âš ï¸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³{i}ã®æœ€åˆã®è¡ŒãŒæ—¥ä»˜å½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“: {first_line}")
                continue

            # æ—¥ä»˜ã®æ¤œè¨¼
            try:
                datetime.strptime(day, "%Y-%m-%d")
            except ValueError:
                print(f"âš ï¸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³{i}ã®æ—¥ä»˜å½¢å¼ãŒä¸æ­£ã§ã™: {day}")
                continue

            # å†…å®¹ã‚’å–å¾—ï¼ˆ2è¡Œç›®ä»¥é™ï¼‰
            content = '\n'.join(lines[1:]).strip()

            print(f"   {day}: {len(content)}æ–‡å­—")
            daily_summaries[day] = content
    else:
        print("[INFO] APIå‘¼ã³å‡ºã—ãŒã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸãŸã‚ã€è¦ç´„å‡¦ç†ã‚‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        daily_summaries = {}

    # å„æ—¥ã‚’Notionã«ç™»éŒ²
    print(f"ğŸ“ Notionç™»éŒ²æ™‚ã®å†…å®¹:")
    print(f"   åˆ©ç”¨å¯èƒ½ãªè¦ç´„çµæœ: {list(daily_summaries.keys())}")
    print(f"   å‡¦ç†å¯¾è±¡ã®æ—¥ä»˜: {list(daily_raw.keys())}")

    # æˆåŠŸã—ãŸæ—¥ä»˜ã‚’è¿½è·¡
    successful_days = []
    failed_days = []

    for day in sorted(daily_raw.keys()):
        print(f"   å‡¦ç†ä¸­ã®æ—¥ä»˜: '{day}'")
        print(f"   daily_summariesã«å­˜åœ¨ã™ã‚‹ã‹: {day in daily_summaries}")

        # è¦ç´„ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if day in daily_summaries:
            summary_content = daily_summaries[day]
            print(f"   è¦ç´„å†…å®¹ã®é•·ã•: {len(summary_content)}")
            print(f"   è¦ç´„å†…å®¹: '{summary_content}'")

            # è¦ç´„å†…å®¹ãŒå®Ÿè³ªçš„ã«ç©ºã§ãªã„ã‹ãƒã‚§ãƒƒã‚¯
            if summary_content.strip() and len(summary_content.strip()) > 5:
                md = f"## {day}\n{summary_content}"  # è¦ç´„çµæœã‚’ä½¿ç”¨
                data_type = "è¦ç´„çµæœ"
                print(f"   â†’ è¦ç´„çµæœã‚’ä½¿ç”¨")
            else:
                md = f"## {day}\n\nä¼šè©±å†…å®¹ãªã—ï¼ˆè¦ç´„ãŒç©ºï¼‰"  # è¦ç´„ãŒç©ºã®å ´åˆ
                data_type = "ä¼šè©±å†…å®¹ãªã—ï¼ˆè¦ç´„ãŒç©ºï¼‰"
                print(f"   â†’ è¦ç´„ãŒç©ºã®ãŸã‚ã€Œä¼šè©±å†…å®¹ãªã—ã€ã‚’ä½¿ç”¨")
        else:
            md = f"## {day}\n\nä¼šè©±å†…å®¹ãªã—ï¼ˆè©²å½“æ—¥ä»˜ã®ãƒ‡ãƒ¼ã‚¿ãªã—ï¼‰"  # è©²å½“æ—¥ä»˜ãªã—
            data_type = "ä¼šè©±å†…å®¹ãªã—ï¼ˆãƒ‡ãƒ¼ã‚¿ãªã—ï¼‰"
            print(f"   â†’ è©²å½“æ—¥ä»˜ã®ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€Œä¼šè©±å†…å®¹ãªã—ã€ã‚’ä½¿ç”¨")

        print(f"   æœ€çµ‚çš„ãªä½¿ç”¨ãƒ‡ãƒ¼ã‚¿: {data_type} ({len(md)}æ–‡å­—)")

        url = notion_create_page(
            notion_token, notion_dbid, title_prop, date_prop, day, md)

        if url:
            print(f"   âœ… Notionä¿å­˜å®Œäº†: {day}")
            print(f"   ğŸ“„ ãƒšãƒ¼ã‚¸URL: {url}")
            successful_days.append(day)
        else:
            print(f"   âŒ Notionä¿å­˜å¤±æ•—: {day}")
            print(f"   âš ï¸ ã‚¨ãƒ©ãƒ¼ã®è©³ç´°ã¯ä¸Šè¨˜ã®ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
            failed_days.append(day)

            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã€ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’åœæ­¢
            print(f"\nğŸš¨ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿã«ã‚ˆã‚Šå‡¦ç†ã‚’åœæ­¢ã—ã¾ã™")
            print(f"   æˆåŠŸã—ãŸæ—¥ä»˜: {successful_days}")
            print(f"   å¤±æ•—ã—ãŸæ—¥ä»˜: {failed_days}")
            print(f"   state.jsonã¯æ›´æ–°ã•ã‚Œã¾ã›ã‚“")
            raise SystemExit(f"Notion API ã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šå‡¦ç†ã‚’åœæ­¢ã—ã¾ã—ãŸã€‚æ—¥ä»˜: {day}")

    # ã™ã¹ã¦æˆåŠŸã—ãŸå ´åˆã®ã¿stateæ›´æ–°
    if successful_days and not failed_days:
        print(f"\nâœ… ã™ã¹ã¦ã®æ—¥ä»˜ã§Notionä¿å­˜ãŒæˆåŠŸã—ã¾ã—ãŸ")
        for conv_id, (new_seen, max_ts) in progress.items():
            state["seen"].setdefault(conv_id, [])
            state["seen"][conv_id].extend(list(new_seen))
            if max_ts > state["conv_hwm"].get(conv_id, -1):
                state["conv_hwm"][conv_id] = max_ts
        save_state(state_path, state)
        print(f"[DONE] æ—¥å ±ç™»éŒ²å®Œäº†ã€‚stateæ›´æ–°æ¸ˆã¿ã€‚")
    else:
        print("[INFO] æ—¥å ±å‡¦ç†ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸã€‚")

    # é€±å ±ä½œæˆãƒã‚§ãƒƒã‚¯
    print("\n=== é€±å ±ä½œæˆãƒã‚§ãƒƒã‚¯ ===")
    workdir = os.getenv("WORK_DIR", "./ChatGPT_Notion")

    # é€±å ±ä½œæˆåˆ¤æ–­ã®è©³ç´°ãƒ­ã‚°
    print("ğŸ” é€±å ±ä½œæˆåˆ¤æ–­ãƒ—ãƒ­ã‚»ã‚¹:")
    last_weekly_date = get_last_weekly_report_date(workdir)
    # æœ€åˆã«èª­ã¿å–ã£ãŸå€¤ã‚’ä¿æŒï¼ˆè¡¨ç¤ºç”¨ï¼‰
    original_last_weekly_date = last_weekly_date
    print(
        f"   æœ€çµ‚ç™»éŒ²æ—¥: {original_last_weekly_date if original_last_weekly_date else 'æœªç™»éŒ²'}")

    today = jst_today()
    today_date = datetime.strptime(today, "%Y-%m-%d").date()
    print(f"   ä»Šæ—¥ã®æ—¥ä»˜: {today}")

    if last_weekly_date:
        last_date = datetime.strptime(last_weekly_date, "%Y-%m-%d").date()
        last_date_weekday = last_date.weekday()
        next_saturday = last_date - \
            timedelta(days=last_date_weekday) + timedelta(days=5)
        if next_saturday <= last_date:
            next_saturday += timedelta(days=7)
        print(f"   å‰å›ç™»éŒ²æ—¥: {last_date}")
        print(f"   æ¬¡ã®åœŸæ›œæ—¥: {next_saturday}")
        print(f"   ä»Šæ—¥ >= æ¬¡ã®åœŸæ›œ: {today_date >= next_saturday}")

        should_create = today_date >= next_saturday
    else:
        print("   æœªç™»éŒ²ã®ãŸã‚ã€å‰ã®é‡‘æ›œæ—¥ã¾ã§ã®ãƒ‡ãƒ¼ã‚¿ã§é€±å ±ä½œæˆã‚’æ¤œè¨")
        should_create = has_sufficient_weekly_data(workdir)

    print(f"   é€±å ±ä½œæˆåˆ¤å®š: {should_create}")

    if should_create:
        print("ğŸ“… é€±å ±ä½œæˆã‚’å®Ÿè¡Œã—ã¾ã™")

        # ä»Šé€±ã®æ—¥ä»˜ç¯„å›²ã‚’å–å¾—
        monday, friday = get_weekly_date_range()
        print(f"   å¯¾è±¡æœŸé–“: {monday} ã€œ {friday}")

        # é€±é–“ã®ç›¸è«‡ãƒ­ã‚°ã‚’å–å¾—ï¼ˆå®Ÿéš›ã®ãƒ­ã‚°ã‹ã‚‰çµ±è¨ˆæƒ…å ±ä»˜ãã§å–å¾—ï¼‰
        weekly_raw_text, weekly_stats = get_weekly_conversations_with_stats(
            conversations, monday, friday)

        print(f"   é€±é–“çµ±è¨ˆ:")
        print(f"     ãƒãƒ£ãƒƒãƒˆä¼šè©±æ•°: {weekly_stats['conversation_count']}å›")
        print(f"     ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°: {weekly_stats['user_message_count']}å›")
        print(f"     ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°: {weekly_stats['assistant_message_count']}å›")
        print(f"     ä¼šè©±æ™‚é–“: {weekly_stats['total_duration_minutes']:.1f}åˆ†")

        # é€±å ±ä½œæˆ
        weekly_report = create_weekly_report(
            weekly_raw_text, openai_key, openai_model)

        if weekly_report:
            # é€±å ±ã‚’Notionã«ç™»éŒ²
            today = jst_today()
            today_date = datetime.strptime(today, "%Y-%m-%d").date()

            # ç™»éŒ²æ—¥ä»˜ã®æ±ºå®š
            if original_last_weekly_date:
                # æ¡ä»¶2: æœ€æ–°ç™»éŒ²æ—¥ä»˜ã®æ¬¡ã®åœŸæ›œæ—¥
                last_date = datetime.strptime(
                    original_last_weekly_date, "%Y-%m-%d").date()
                last_date_weekday = last_date.weekday()
                next_saturday = last_date - \
                    timedelta(days=last_date_weekday) + timedelta(days=5)
                if next_saturday <= last_date:
                    next_saturday += timedelta(days=7)
                # åœŸæ›œæ—¥ã‚’è¨ˆç®—ã—ã¦ã‹ã‚‰æ—¥æ›œæ—¥ã«å¤‰æ›´
                registration_date = next_saturday + timedelta(days=1)
                print(f"   ç™»éŒ²æ—¥ä»˜: {registration_date} (å‰å›ç™»éŒ²æ—¥{last_date}ã®æ¬¡ã®æ—¥æ›œæ—¥)")
            else:
                # æ¡ä»¶1: ã‚·ã‚¹ãƒ†ãƒ æ—¥ä»˜ç›´è¿‘ã®é‡‘æ›œæ—¥ï¼ˆã‚·ã‚¹ãƒ†ãƒ æ—¥ä»˜ãŒé‡‘æ›œã®å ´åˆã¯ç™»éŒ²ã—ãªã„ï¼‰
                days_since_monday = today_date.weekday()
                monday = today_date - timedelta(days=days_since_monday)
                friday = monday + timedelta(days=4)

                if today_date == friday:
                    print("   ä»Šæ—¥ãŒé‡‘æ›œæ—¥ã®ãŸã‚ã€é€±å ±ç™»éŒ²ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                    return

                # é‡‘æ›œæ—¥ã‚’è¨ˆç®—ã—ã¦ã‹ã‚‰æ—¥æ›œæ—¥ã«å¤‰æ›´
                registration_date = friday + timedelta(days=2)
                print(f"   ç™»éŒ²æ—¥ä»˜: {registration_date} (ç›´è¿‘ã®æ—¥æ›œæ—¥)")

            weekly_title = f"{registration_date} é€±é–“å­¦ç¿’ãƒ¬ãƒãƒ¼ãƒˆ"

            # UTCã§æ—¥æœ¬æ™‚é–“12-13æ™‚ã‚’æŒ‡å®šï¼ˆUTC = JST - 9æ™‚é–“ï¼‰
            start_time_utc = datetime.combine(
                registration_date, datetime.min.time().replace(hour=3, minute=0)).replace(tzinfo=timezone.utc)  # JST 12:00 = UTC 03:00
            end_time_utc = datetime.combine(
                registration_date, datetime.min.time().replace(hour=4, minute=0)).replace(tzinfo=timezone.utc)  # JST 13:00 = UTC 04:00

            weekly_date_with_time = {
                "start": start_time_utc.isoformat(),
                "end": end_time_utc.isoformat(),
                "time_zone": "Asia/Tokyo"
            }

            try:
                weekly_page = notion_create_weekly_page(notion_token, notion_dbid, title_prop,
                                                        date_prop, weekly_title, weekly_date_with_time, weekly_report)

                if weekly_page and weekly_page.get("url"):
                    # æœ€çµ‚é€±å ±ç™»éŒ²æ—¥ã‚’ä¿å­˜ï¼ˆç™»éŒ²æ—¥ä»˜ã‚’ä½¿ç”¨ï¼‰
                    save_last_weekly_report_date(
                        workdir, registration_date.isoformat())

                    print("âœ… é€±å ±ä¿å­˜å®Œäº†")
                    print("é€±å ±ãƒšãƒ¼ã‚¸URL:", weekly_page.get("url"))
                else:
                    print("âŒ é€±å ±ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
                    print("âš ï¸ ã‚¨ãƒ©ãƒ¼ã®è©³ç´°ã¯ä¸Šè¨˜ã®ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
                    raise SystemExit("é€±å ±ä½œæˆã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šå‡¦ç†ã‚’åœæ­¢ã—ã¾ã—ãŸ")
            except Exception as e:
                print(f"âŒ é€±å ±ä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                raise SystemExit(f"é€±å ±ä½œæˆã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šå‡¦ç†ã‚’åœæ­¢ã—ã¾ã—ãŸ: {e}")
        else:
            print("âš ï¸ é€±å ±ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
    else:
        print("ğŸ“… é€±å ±ä½œæˆã¯ä¸è¦ã§ã™")

    print(f"[DONE] å…¨å‡¦ç†å®Œäº†")


if __name__ == "__main__":
    main()
